# ABPHA-case-study

Bias is an important measure to ensure accurate and fair machine learning implementations. In cases where bias does exist, bias can cause misdiagnoses as well as unequal distribution of research in public health.
With the new European Union AI act, the focus on detecting bias becomes more prevalent. However, they are not often actively considered in research implementations.
This thesis dives into four recent papers that were published between 2019 and 2023, use DHS datasets, and are conducted in Bangladesh. With these four papers, this thesis researches to what extent bias can be identified, and mitigated within machine learning applications in scientific public health studies. Next to that, this research looks into the adherence of principles of fairness by these papers, and combines the results in a practical experiment.
To determine the extent of bias within the papers this research looked for four types of bias: (1) measurement, (2) non-response, (3) omitted variable, and (4) sampling bias. 
Using the results of the bias evaluation in combination with literature, mitigation strategies to counter found bias  are proposed. Fairness is assessed by: (1) equalized odds, (2) equal opportunity, and (3) fairness through unawareness.
Within the papers several types of bias are found. Mitigation strategies are proposed for all studied types of bias. The papers which implement binary classification do not manage to satisfy the requirements for equalized odds and do not mention  fairness in their papers.
